{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f85693aa-eb89-4228-ae15-587342ee1216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "import models\n",
    "import dataprep\n",
    "import evalmetrics\n",
    "import torch.nn as nn\n",
    "import custom_dataset\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f4385b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking if GPU is available\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "569c0303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing Train Test and Validation Data\n",
    "\n",
    "data_dir = 'SWED_sample/train/'\n",
    "dataset = custom_dataset.CustomDataset(data_dir)\n",
    "\n",
    "#Using the split 80% train 10% validation and 10% test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "#train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a563c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the subset splits\n",
    "\n",
    "# Save the subsets\n",
    "torch.save(train_dataset, 'trainc_dataset.pth')\n",
    "torch.save(val_dataset, 'valc_dataset.pth')\n",
    "torch.save(test_dataset, 'testc_dataset.pth')\n",
    "\n",
    "# Load the subset\n",
    "#loaded_subset_dataset = torch.load('subset_dataset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4981ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset splits (if already saved)\n",
    "\n",
    "train_dataset = torch.load('trainc_dataset.pth')\n",
    "val_dataset = torch.load('valc_dataset.pth')\n",
    "test_dataset = torch.load('testc_dataset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aa0fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Dataloaders compatible with keras models\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0674999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Train Loss: 0.1794, Val Loss: 0.0564\n",
      "Epoch [2/30], Train Loss: 0.0771, Val Loss: 0.0494\n",
      "Epoch [3/30], Train Loss: 0.0698, Val Loss: 0.0939\n",
      "Epoch [4/30], Train Loss: 0.0545, Val Loss: 0.0401\n",
      "Epoch [5/30], Train Loss: 0.0470, Val Loss: 0.0325\n",
      "Epoch [6/30], Train Loss: 0.0506, Val Loss: 0.0261\n",
      "Epoch [7/30], Train Loss: 0.0338, Val Loss: 0.0253\n",
      "Epoch [8/30], Train Loss: 0.0411, Val Loss: 0.0230\n",
      "Epoch [9/30], Train Loss: 0.0369, Val Loss: 0.0261\n",
      "Epoch [10/30], Train Loss: 0.0358, Val Loss: 0.0237\n",
      "Epoch [11/30], Train Loss: 0.0361, Val Loss: 0.0231\n",
      "Epoch [12/30], Train Loss: 0.0353, Val Loss: 0.0244\n",
      "Epoch [13/30], Train Loss: 0.0354, Val Loss: 0.0283\n",
      "Epoch [14/30], Train Loss: 0.0338, Val Loss: 0.0494\n",
      "Epoch [15/30], Train Loss: 0.0303, Val Loss: 0.0215\n",
      "Epoch [16/30], Train Loss: 0.0278, Val Loss: 0.0208\n",
      "Epoch [17/30], Train Loss: 0.0300, Val Loss: 0.0338\n",
      "Epoch [18/30], Train Loss: 0.0294, Val Loss: 0.0210\n",
      "Epoch [19/30], Train Loss: 0.0374, Val Loss: 0.0232\n",
      "Epoch [20/30], Train Loss: 0.0292, Val Loss: 0.0248\n",
      "Epoch [21/30], Train Loss: 0.0270, Val Loss: 0.0198\n",
      "Epoch [22/30], Train Loss: 0.0263, Val Loss: 0.0201\n",
      "Epoch [23/30], Train Loss: 0.0250, Val Loss: 0.0211\n",
      "Epoch [24/30], Train Loss: 0.0246, Val Loss: 0.0273\n",
      "Epoch [25/30], Train Loss: 0.0259, Val Loss: 0.0838\n",
      "Epoch [26/30], Train Loss: 0.0361, Val Loss: 0.0206\n",
      "Epoch [27/30], Train Loss: 0.0245, Val Loss: 0.0257\n",
      "Epoch [28/30], Train Loss: 0.0275, Val Loss: 0.0198\n",
      "Epoch [29/30], Train Loss: 0.0254, Val Loss: 0.1913\n",
      "Epoch [30/30], Train Loss: 0.0275, Val Loss: 0.0201\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#model = models.AttU_Net(img_ch=3,output_ch=1)\n",
    "\n",
    "## LOADING EUNET \n",
    "#model = models.U_Net(img_ch=3,output_ch=1)\n",
    "model = models.EdgeU1_Net(img_ch=3,output_ch=1)\n",
    "        \n",
    "# Defining  loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "num_epochs = 30\n",
    "\n",
    "# Set the device to use for training\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "######################## Training model on train dataset #####################################\n",
    "\n",
    "# Train the model\n",
    "model.to(device)\n",
    "\n",
    "#Tracking Losses\n",
    "losst_list = []\n",
    "lossv_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    #Train the model\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        #print(inputs.shape)\n",
    "        labels = labels.to(device)\n",
    "        #print('labels are')\n",
    "        #print(labels.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    losst_list.append(avg_train_loss)\n",
    "            \n",
    "    #Validate the training process\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "    lossv_list.append(avg_val_loss)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "        \n",
    "        #print(f\"Epoch number{i}\")\n",
    "        #print(f\"[Accuracy]{evalmetrics.get_accuracy(outputs,labels,threshold=0.5)}\")\n",
    "        #print(f\"[Sensitivity]{evalmetrics.get_sensitivity(outputs,labels,threshold=0.5)}\")\n",
    "        #print(f\"[Specificity]{evalmetrics.get_specificity(outputs,labels,threshold=0.5)}\")\n",
    "        #print(f\"[Precision]{evalmetrics.get_precision(outputs,labels,threshold=0.5)}\")\n",
    "        #print(f\"[F1]{evalmetrics.get_F1(outputs,labels,threshold=0.5)}\")\n",
    "        #print(f\"[JS]{evalmetrics.get_JS(outputs,labels,threshold=0.5)}\") \n",
    "        #print(f\"[DC]{evalmetrics.get_DC(outputs,labels,threshold=0.5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919fbc71",
   "metadata": {},
   "source": [
    "# Save and Load model & Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df836996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# Open a file for writing\n",
    "with open('train_losses_eunet2.pkl', 'wb') as f:\n",
    "    # Use pickle to dump the list to the file\n",
    "    pickle.dump(losst_list, f)\n",
    "    \n",
    "\n",
    "# Open a file for writing\n",
    "with open('val_losses_eunet2.pkl', 'wb') as f:\n",
    "    # Use pickle to dump the list to the file\n",
    "    pickle.dump(lossv_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bd91bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epoch,\n",
    "    # add any other relevant information\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'eunet_trained2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d59acebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fac615",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losst_list, label='Train Loss')\n",
    "plt.plot(lossv_list, label='Validation Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('AttU Net')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ca600",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20bc0977",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e3d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Load AU-Net\n",
    "checkpoint = torch.load('aunet_trained2.pth',map_location=torch.device('cpu'))\n",
    "\n",
    "# Create a new instance of your model\n",
    "model = models.AttU_Net(img_ch=3,output_ch=1)\n",
    "model.to(device)\n",
    "\n",
    "# Load the model state from the checkpoint\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Load the optimizer state from the checkpoint, if needed\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.to(device)\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b940a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.to(device)\n",
    "\n",
    "# Initialize accumulators for each metric\n",
    "sensitivity_sum = 0\n",
    "specificity_sum = 0\n",
    "precision_sum = 0\n",
    "F1_sum = 0\n",
    "JS_sum = 0\n",
    "DC_sum = 0\n",
    "num_batches = 0\n",
    "\n",
    "# Loop over the test dataset\n",
    "for data, labels in test_loader:\n",
    "    # Perform inference\n",
    "    \n",
    "    \n",
    "    data = data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    preds = model(data)\n",
    "\n",
    "    # Evaluate the predictions\n",
    "    sensitivity = evalmetrics.get_sensitivity(preds, labels)\n",
    "    specificity = evalmetrics.get_specificity(preds, labels)\n",
    "    precision = evalmetrics.get_precision(preds, labels)\n",
    "    F1 = evalmetrics.get_F1(preds, labels)\n",
    "    JS = evalmetrics.get_JS(preds, labels)\n",
    "    DC = evalmetrics.get_DC(preds, labels)\n",
    "\n",
    "    # Accumulate the metric values\n",
    "    sensitivity_sum += sensitivity * len(data)\n",
    "    specificity_sum += specificity * len(data)\n",
    "    precision_sum += precision * len(data)\n",
    "    F1_sum += F1 * len(data)\n",
    "    JS_sum += JS * len(data)\n",
    "    DC_sum += DC * len(data)\n",
    "    num_batches += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c16d4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sensitivity: 0.9160\n",
      "Average Specificity: 0.9977\n",
      "Average Precision: 0.9537\n",
      "Average F1 Score: 0.9293\n",
      "Average Jaccard Similarity: 0.8957\n",
      "Average Dice Coefficient: 0.9293\n"
     ]
    }
   ],
   "source": [
    "# Compute the average of each metric\n",
    "sensitivity_avg = sensitivity_sum / len(test_loader.dataset)\n",
    "specificity_avg = specificity_sum / len(test_loader.dataset)\n",
    "precision_avg = precision_sum / len(test_loader.dataset)\n",
    "F1_avg = F1_sum / len(test_loader.dataset)\n",
    "JS_avg = JS_sum / len(test_loader.dataset)\n",
    "DC_avg = DC_sum / len(test_loader.dataset)\n",
    "\n",
    "# Print the average of each metric\n",
    "print(\"Average Sensitivity: {:.4f}\".format(sensitivity_avg))\n",
    "print(\"Average Specificity: {:.4f}\".format(specificity_avg))\n",
    "print(\"Average Precision: {:.4f}\".format(precision_avg))\n",
    "print(\"Average F1 Score: {:.4f}\".format(F1_avg))\n",
    "print(\"Average Jaccard Similarity: {:.4f}\".format(JS_avg))\n",
    "print(\"Average Dice Coefficient: {:.4f}\".format(DC_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1814f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
