{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a10bd02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "import models\n",
    "import dataprep\n",
    "import evalmetrics\n",
    "import torch.nn as nn\n",
    "import custom_dataset\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ce0e3",
   "metadata": {},
   "source": [
    "# Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27719486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "U_Net(\n",
       "  (Maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Conv1): conv_block(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Conv2): conv_block(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Conv3): conv_block(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Conv4): conv_block(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Conv5): conv_block(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Up5): up_conv(\n",
       "    (up): Sequential(\n",
       "      (0): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Up_conv5): conv_block(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Up4): up_conv(\n",
       "    (up): Sequential(\n",
       "      (0): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Up_conv4): conv_block(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Up3): up_conv(\n",
       "    (up): Sequential(\n",
       "      (0): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Up_conv3): conv_block(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Up2): up_conv(\n",
       "    (up): Sequential(\n",
       "      (0): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Up_conv2): conv_block(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Conv_1x1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For inference, we can still to cpu\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "################# Load EU-Net\n",
    "checkpoint = torch.load('eunet_trained2.pth',map_location=torch.device('cpu'))\n",
    "\n",
    "# Create a new instance of your model\n",
    "model_e = models.EdgeU1_Net(img_ch=3,output_ch=1)\n",
    "model_e.to(device)\n",
    "\n",
    "# Load the model state from the checkpoint\n",
    "model_e.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Load the optimizer state from the checkpoint, if needed\n",
    "optimizer_e = torch.optim.Adam(model_e.parameters())\n",
    "optimizer_e.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model_e.to(device)\n",
    "model_e.eval() \n",
    "\n",
    "################# Load AU-Net\n",
    "checkpoint = torch.load('aunet_trained2.pth',map_location=torch.device('cpu'))\n",
    "\n",
    "# Create a new instance of your model\n",
    "model_a = models.AttU_Net(img_ch=3,output_ch=1)\n",
    "model_a.to(device)\n",
    "\n",
    "# Load the model state from the checkpoint\n",
    "model_a.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Load the optimizer state from the checkpoint, if needed\n",
    "optimizer_a = torch.optim.Adam(model_a.parameters())\n",
    "optimizer_a.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model_a.to(device)\n",
    "model_a.eval()\n",
    "\n",
    "################# Load U-Net\n",
    "checkpoint = torch.load('unet_trained2.pth',map_location=torch.device('cpu'))\n",
    "\n",
    "# Create a new instance of your model\n",
    "model_u = models.U_Net(img_ch=3,output_ch=1)\n",
    "model_u.to(device)\n",
    "\n",
    "# Load the model state from the checkpoint\n",
    "model_u.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Load the optimizer state from the checkpoint, if needed\n",
    "optimizer_u = torch.optim.Adam(model_u.parameters())\n",
    "optimizer_u.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model_u.to(device)\n",
    "model_u.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792fd2f8",
   "metadata": {},
   "source": [
    "# Loading Test dataset and preparing DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03402efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torch.load('testc_dataset.pth')\n",
    "test_loader = DataLoader(test_dataset, batch_size = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58689719",
   "metadata": {},
   "source": [
    "### Running U-Net on test data and evaluating metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8434dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sensitivity: 0.8886\n",
      "Average Specificity: 0.9993\n",
      "Average Precision: 0.9888\n",
      "Average F1 Score: 0.9186\n",
      "Average Jaccard Similarity: 0.8822\n",
      "Average Dice Coefficient: 0.9186\n"
     ]
    }
   ],
   "source": [
    "model_u.to(device)\n",
    "\n",
    "# Initialize accumulators for each metric\n",
    "sensitivity_sum = 0\n",
    "specificity_sum = 0\n",
    "precision_sum = 0\n",
    "F1_sum = 0\n",
    "JS_sum = 0\n",
    "DC_sum = 0\n",
    "num_batches = 0\n",
    "\n",
    "# Loop over the test dataset\n",
    "for data, labels in test_loader:\n",
    "    # Perform inference\n",
    "    \n",
    "    \n",
    "    data = data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    preds = model_u(data)\n",
    "\n",
    "    # Evaluate the predictions\n",
    "    sensitivity = evalmetrics.get_sensitivity(preds, labels)\n",
    "    specificity = evalmetrics.get_specificity(preds, labels)\n",
    "    precision = evalmetrics.get_precision(preds, labels)\n",
    "    F1 = evalmetrics.get_F1(preds, labels)\n",
    "    JS = evalmetrics.get_JS(preds, labels)\n",
    "    DC = evalmetrics.get_DC(preds, labels)\n",
    "\n",
    "    # Accumulate the metric values\n",
    "    sensitivity_sum += sensitivity * len(data)\n",
    "    specificity_sum += specificity * len(data)\n",
    "    precision_sum += precision * len(data)\n",
    "    F1_sum += F1 * len(data)\n",
    "    JS_sum += JS * len(data)\n",
    "    DC_sum += DC * len(data)\n",
    "    num_batches += 1\n",
    "\n",
    "# Compute the average of each metric\n",
    "sensitivity_avg_u = sensitivity_sum / len(test_loader.dataset)\n",
    "specificity_avg_u = specificity_sum / len(test_loader.dataset)\n",
    "precision_avg_u = precision_sum / len(test_loader.dataset)\n",
    "F1_avg_u = F1_sum / len(test_loader.dataset)\n",
    "JS_avg_u = JS_sum / len(test_loader.dataset)\n",
    "DC_avg_u = DC_sum / len(test_loader.dataset)\n",
    "\n",
    "# Print the average of each metric\n",
    "print(\"Average Sensitivity: {:.4f}\".format(sensitivity_avg_u))\n",
    "print(\"Average Specificity: {:.4f}\".format(specificity_avg_u))\n",
    "print(\"Average Precision: {:.4f}\".format(precision_avg_u))\n",
    "print(\"Average F1 Score: {:.4f}\".format(F1_avg_u))\n",
    "print(\"Average Jaccard Similarity: {:.4f}\".format(JS_avg_u))\n",
    "print(\"Average Dice Coefficient: {:.4f}\".format(DC_avg_u))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b792d0da",
   "metadata": {},
   "source": [
    "### Running AU-Net on test data and evaluating metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ec7eca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sensitivity: 0.9160\n",
      "Average Specificity: 0.9977\n",
      "Average Precision: 0.9537\n",
      "Average F1 Score: 0.9293\n",
      "Average Jaccard Similarity: 0.8957\n",
      "Average Dice Coefficient: 0.9293\n"
     ]
    }
   ],
   "source": [
    "model_a.to(device)\n",
    "\n",
    "# Initialize accumulators for each metric\n",
    "sensitivity_sum = 0\n",
    "specificity_sum = 0\n",
    "precision_sum = 0\n",
    "F1_sum = 0\n",
    "JS_sum = 0\n",
    "DC_sum = 0\n",
    "num_batches = 0\n",
    "\n",
    "# Loop over the test dataset\n",
    "for data, labels in test_loader:\n",
    "    # Perform inference\n",
    "    data = data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    preds = model_a(data)\n",
    "\n",
    "    # Evaluate the predictions\n",
    "    sensitivity = evalmetrics.get_sensitivity(preds, labels)\n",
    "    specificity = evalmetrics.get_specificity(preds, labels)\n",
    "    precision = evalmetrics.get_precision(preds, labels)\n",
    "    F1 = evalmetrics.get_F1(preds, labels)\n",
    "    JS = evalmetrics.get_JS(preds, labels)\n",
    "    DC = evalmetrics.get_DC(preds, labels)\n",
    "\n",
    "    # Accumulate the metric values\n",
    "    sensitivity_sum += sensitivity * len(data)\n",
    "    specificity_sum += specificity * len(data)\n",
    "    precision_sum += precision * len(data)\n",
    "    F1_sum += F1 * len(data)\n",
    "    JS_sum += JS * len(data)\n",
    "    DC_sum += DC * len(data)\n",
    "    num_batches += 1\n",
    "\n",
    "# Compute the average of each metric\n",
    "sensitivity_avg_a = sensitivity_sum / len(test_loader.dataset)\n",
    "specificity_avg_a = specificity_sum / len(test_loader.dataset)\n",
    "precision_avg_a = precision_sum / len(test_loader.dataset)\n",
    "F1_avg_a = F1_sum / len(test_loader.dataset)\n",
    "JS_avg_a = JS_sum / len(test_loader.dataset)\n",
    "DC_avg_a = DC_sum / len(test_loader.dataset)\n",
    "\n",
    "# Print the average of each metric\n",
    "print(\"Average Sensitivity: {:.4f}\".format(sensitivity_avg_a))\n",
    "print(\"Average Specificity: {:.4f}\".format(specificity_avg_a))\n",
    "print(\"Average Precision: {:.4f}\".format(precision_avg_a))\n",
    "print(\"Average F1 Score: {:.4f}\".format(F1_avg_a))\n",
    "print(\"Average Jaccard Similarity: {:.4f}\".format(JS_avg_a))\n",
    "print(\"Average Dice Coefficient: {:.4f}\".format(DC_avg_a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3887630",
   "metadata": {},
   "source": [
    "### Running EU-Net on test data and evaluating metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ecab2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sensitivity: 0.9123\n",
      "Average Specificity: 0.9987\n",
      "Average Precision: 0.9788\n",
      "Average F1 Score: 0.9358\n",
      "Average Jaccard Similarity: 0.9004\n",
      "Average Dice Coefficient: 0.9358\n"
     ]
    }
   ],
   "source": [
    "model_e.to(device)\n",
    "\n",
    "# Initialize accumulators for each metric\n",
    "sensitivity_sum = 0\n",
    "specificity_sum = 0\n",
    "precision_sum = 0\n",
    "F1_sum = 0\n",
    "JS_sum = 0\n",
    "DC_sum = 0\n",
    "num_batches = 0\n",
    "\n",
    "# Loop over the test dataset\n",
    "for data, labels in test_loader:\n",
    "    # Perform inference\n",
    "    \n",
    "    data = data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    preds = model_e(data)\n",
    "\n",
    "    # Evaluate the predictions\n",
    "    sensitivity = evalmetrics.get_sensitivity(preds, labels)\n",
    "    specificity = evalmetrics.get_specificity(preds, labels)\n",
    "    precision = evalmetrics.get_precision(preds, labels)\n",
    "    F1 = evalmetrics.get_F1(preds, labels)\n",
    "    JS = evalmetrics.get_JS(preds, labels)\n",
    "    DC = evalmetrics.get_DC(preds, labels)\n",
    "\n",
    "    # Accumulate the metric values\n",
    "    sensitivity_sum += sensitivity * len(data)\n",
    "    specificity_sum += specificity * len(data)\n",
    "    precision_sum += precision * len(data)\n",
    "    F1_sum += F1 * len(data)\n",
    "    JS_sum += JS * len(data)\n",
    "    DC_sum += DC * len(data)\n",
    "    num_batches += 1\n",
    "\n",
    "# Compute the average of each metric\n",
    "sensitivity_avg_e = sensitivity_sum / len(test_loader.dataset)\n",
    "specificity_avg_e = specificity_sum / len(test_loader.dataset)\n",
    "precision_avg_e = precision_sum / len(test_loader.dataset)\n",
    "F1_avg_e = F1_sum / len(test_loader.dataset)\n",
    "JS_avg_e = JS_sum / len(test_loader.dataset)\n",
    "DC_avg_e = DC_sum / len(test_loader.dataset)\n",
    "\n",
    "# Print the average of each metric\n",
    "print(\"Average Sensitivity: {:.4f}\".format(sensitivity_avg_e))\n",
    "print(\"Average Specificity: {:.4f}\".format(specificity_avg_e))\n",
    "print(\"Average Precision: {:.4f}\".format(precision_avg_e))\n",
    "print(\"Average F1 Score: {:.4f}\".format(F1_avg_e))\n",
    "print(\"Average Jaccard Similarity: {:.4f}\".format(JS_avg_e))\n",
    "print(\"Average Dice Coefficient: {:.4f}\".format(DC_avg_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63f70a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    model_u             model_a             model_e             \n",
      "Sensitivity         0.8886              0.9160              0.9123              \n",
      "Specificity         0.9993              0.9977              0.9987              \n",
      "Precision           0.9888              0.9537              0.9788              \n",
      "F1 Score            0.9186              0.9293              0.9358              \n",
      "Jaccard Similarity  0.8822              0.8957              0.9004              \n",
      "Dice Coefficient    0.9186              0.9293              0.9358              \n"
     ]
    }
   ],
   "source": [
    "# Define the metric names\n",
    "metric_names = ['Sensitivity', 'Specificity', 'Precision', 'F1 Score', 'Jaccard Similarity', 'Dice Coefficient']\n",
    "\n",
    "# Define the model names\n",
    "model_names = ['model_u', 'model_a', 'model_e']\n",
    "\n",
    "# Define a dictionary to store the metric values for each model\n",
    "model_metrics = {\n",
    "    'model_u': [sensitivity_avg_u, specificity_avg_u, precision_avg_u, F1_avg_u, JS_avg_u, DC_avg_u],\n",
    "    'model_a': [sensitivity_avg_a, specificity_avg_a, precision_avg_a, F1_avg_a, JS_avg_a, DC_avg_a],\n",
    "    'model_e': [sensitivity_avg_e, specificity_avg_e, precision_avg_e, F1_avg_e, JS_avg_e, DC_avg_e]\n",
    "}\n",
    "\n",
    "# Print the table\n",
    "print(\"{:<20}{:<20}{:<20}{:<20}\".format('', *model_names))\n",
    "for i in range(len(metric_names)):\n",
    "    print(\"{:<20}{:<20.4f}{:<20.4f}{:<20.4f}\".format(metric_names[i], model_metrics[model_names[0]][i], model_metrics[model_names[1]][i], model_metrics[model_names[2]][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcc2caf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
